      "metadata": {},
        "# FARS Accident Severity Classification\n",
        "\n",
        "This notebook explores the **FARS** dataset and builds multiple machine learning pipelines to classify accident severity. It includes:\n",
        "- Exploratory data analysis (EDA)\n",
        "- Data preprocessing\n",
        "- Cross-validated model evaluation\n",
        "- Hyperparameter tuning for selected pipelines\n",
        "- A short discussion of results\n",
        "\n",
        "> **Note:** `fars.csv` should be located in the same folder as this notebook.\n"
      "metadata": {},
      "execution_count": null,
        "\n",
        "from sklearn.model_selection import StratifiedKFold, cross_validate, GridSearchCV, cross_val_predict\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n"
      "metadata": {},
      "execution_count": null,
      "outputs": [],
        "# Load dataset\n",
        "file_path = \"fars.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "df.head()\n"
      "cell_type": "markdown",
      "metadata": {},
        "## Exploratory Data Analysis (EDA)\n",
        "We review data types, missing values, and the distribution of the target class."
      "metadata": {},
      "execution_count": null,
      "outputs": [],
        "# Basic info and missing values\n",
        "print(df.info())\n",
        "\n",
        "missing = df.isnull().sum().sort_values(ascending=False)\n",
        "print(\"\\nMissing Values (Top 10):\")\n",
        "print(missing.head(10))\n",
        "print(f\"\\nTotal missing values: {missing.sum()}\")\n",
        "print(f\"Percentage of missing values: {(missing.sum() / (df.shape[0] * df.shape[1]) * 100):.2f}%\")\n",
        "\n",
        "# Target column (last column)\n",
        "target_col = df.columns[-1]\n",
        "print(f\"\\nTarget Column: {target_col}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Split features and target\n",
        "X = df.drop(columns=[target_col])\n",
        "y = df[target_col]\n",
        "\n",
        "# Identify numeric and categorical features\n",
        "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_features = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "print(f\"Numeric features ({len(numeric_features)}): {numeric_features}\")\n",
        "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Class distribution\n",
        "class_counts = y.value_counts().sort_index()\n",
        "class_perc = y.value_counts(normalize=True).sort_index() * 100\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "sns.barplot(x=class_counts.index.astype(str), y=class_counts.values, ax=axes[0], palette=\"viridis\")\n",
        "axes[0].set_title(\"Class Distribution (Count)\")\n",
        "axes[0].set_xlabel(\"Severity Class\")\n",
        "axes[0].set_ylabel(\"Count\")\n",
        "\n",
        "sns.barplot(x=class_perc.index.astype(str), y=class_perc.values, ax=axes[1], palette=\"viridis\")\n",
        "axes[1].set_title(\"Class Distribution (Percentage)\")\n",
        "axes[1].set_xlabel(\"Severity Class\")\n",
        "axes[1].set_ylabel(\"Percentage\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Summary statistics for numeric features\n",
        "if numeric_features:\n",
        "    display(df[numeric_features].describe().T)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Missing values by feature (top 15)\n",
        "missing_top = missing[missing > 0].head(15)\n",
        "if not missing_top.empty:\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    sns.barplot(x=missing_top.values, y=missing_top.index, palette=\"magma\")\n",
        "    plt.title(\"Top Missing Values by Feature\")\n",
        "    plt.xlabel(\"Missing Count\")\n",
        "    plt.ylabel(\"Feature\")\n",
        "    plt.show()\n"
      "metadata": {},
      "execution_count": null,
      "outputs": [],
        "# Correlation heatmap for numeric features\n",
        "if len(numeric_features) > 1:\n",
        "    corr = df[numeric_features].corr()\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr, cmap=\"coolwarm\", center=0)\n",
        "    plt.title(\"Numeric Feature Correlation\")\n",
        "    plt.show()\n"
      "metadata": {},
        "## Data Preprocessing\n",
        "We apply:\n",
        "- **Imputation** (median for numeric, most frequent for categorical)\n",
        "- **One-hot encoding** for categorical variables\n",
        "- **Scaling** for numeric features (Standard or Robust, depending on pipeline)\n",
        "- **Optional feature selection** for linear models\n"
      ]
      "metadata": {},
      "execution_count": null,
      "outputs": [],
        "numeric_standard = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "numeric_robust = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", RobustScaler())\n",
        "])\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "preprocess_standard = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_standard, numeric_features),\n",
        "        (\"cat\", categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "preprocess_robust = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_robust, numeric_features),\n",
        "        (\"cat\", categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n"
      "metadata": {},
        "## Model Pipelines and Cross-Validation\n",
        "We evaluate **four** classification pipelines using **Stratified K-Fold cross-validation** and multiple metrics to handle potential class imbalance.\n"
      ]
      "metadata": {},
      "execution_count": null,
      "outputs": [],
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "pipelines = {\n",
        "    \"Logistic Regression + SelectKBest\": Pipeline(steps=[\n",
        "        (\"preprocess\", preprocess_standard),\n",
        "        (\"select\", SelectKBest(score_func=f_classif, k=20)),\n",
        "        (\"model\", LogisticRegression(max_iter=2000, class_weight=\"balanced\"))\n",
        "    ]),\n",
        "    \"Random Forest\": Pipeline(steps=[\n",
        "        (\"preprocess\", preprocess_standard),\n",
        "        (\"model\", RandomForestClassifier(\n",
        "            n_estimators=300,\n",
        "            random_state=42,\n",
        "            class_weight=\"balanced\"\n",
        "        ))\n",
        "    ]),\n",
        "    \"Gradient Boosting\": Pipeline(steps=[\n",
        "        (\"preprocess\", preprocess_standard),\n",
        "        (\"model\", GradientBoostingClassifier(random_state=42))\n",
        "    ]),\n",
        "    \"SVM (RBF + Robust Scaling)\": Pipeline(steps=[\n",
        "        (\"preprocess\", preprocess_robust),\n",
        "        (\"model\", SVC(kernel=\"rbf\", class_weight=\"balanced\", probability=True))\n",
        "    ])\n",
        "}\n",
        "scoring = {\n",
        "    \"accuracy\": \"accuracy\",\n",
        "    \"balanced_accuracy\": \"balanced_accuracy\",\n",
        "    \"precision_macro\": \"precision_macro\",\n",
        "    \"recall_macro\": \"recall_macro\",\n",
        "    \"f1_macro\": \"f1_macro\",\n",
        "    \"roc_auc_ovr\": \"roc_auc_ovr\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Cross-validated evaluation\n",
        "results = []\n",
        "for name, pipe in pipelines.items():\n",
        "    cv_results = cross_validate(pipe, X, y, cv=cv, scoring=scoring, n_jobs=-1)\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": cv_results[\"test_accuracy\"].mean(),\n",
        "        \"Balanced Accuracy\": cv_results[\"test_balanced_accuracy\"].mean(),\n",
        "        \"Precision Macro\": cv_results[\"test_precision_macro\"].mean(),\n",
        "        \"Recall Macro\": cv_results[\"test_recall_macro\"].mean(),\n",
        "        \"F1 Macro\": cv_results[\"test_f1_macro\"].mean(),\n",
        "        \"ROC AUC OVR\": cv_results[\"test_roc_auc_ovr\"].mean()\n",
        "    })\n",
        "results_df = pd.DataFrame(results).sort_values(by=\"F1 Macro\", ascending=False)\n",
        "results_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameter Tuning\n",
        "We perform **Grid Search** on two pipelines to explore improved configurations.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Grid search for Logistic Regression + SelectKBest\n",
        "log_reg_grid = {\n",
        "    \"select__k\": [10, 20, 40],\n",
        "    \"model__C\": [0.1, 1.0, 10.0],\n",
        "    \"model__penalty\": [\"l2\"],\n",
        "    \"model__solver\": [\"lbfgs\"]\n",
        "}\n",
        "log_reg_search = GridSearchCV(\n",
        "    pipelines[\"Logistic Regression + SelectKBest\"],\n",
        "    param_grid=log_reg_grid,\n",
        "    scoring=\"f1_macro\",\n",
        "    cv=cv,\n",
        "    n_jobs=-1\n",
        "log_reg_search.fit(X, y)\n",
        "print(\"Best Logistic Regression Params:\", log_reg_search.best_params_)\n",
        "print(\"Best Logistic Regression F1 Macro:\", log_reg_search.best_score_)\n"
      "metadata": {},
      "execution_count": null,
      "outputs": [],
        "# Grid search for Random Forest\n",
        "rf_grid = {\n",
        "    \"model__n_estimators\": [200, 400],\n",
        "    \"model__max_depth\": [None, 10, 20],\n",
        "    \"model__min_samples_split\": [2, 5]\n",
        "}\n",
        "rf_search = GridSearchCV(\n",
        "    pipelines[\"Random Forest\"],\n",
        "    param_grid=rf_grid,\n",
        "    scoring=\"f1_macro\",\n",
        "    cv=cv,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_search.fit(X, y)\n",
        "print(\"Best Random Forest Params:\", rf_search.best_params_)\n",
        "print(\"Best Random Forest F1 Macro:\", rf_search.best_score_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detailed Report for Best Model\n",
        "We generate cross-validated predictions for the top-performing model by **F1 Macro**.\n"
      "metadata": {},
      "execution_count": null,
      "outputs": [],
        "# Select best model by F1 Macro from initial CV\n",
        "best_model_name = results_df.iloc[0][\"Model\"]\n",
        "print(\"Best baseline model:\", best_model_name)\n",
        "best_model = pipelines[best_model_name]\n",
        "# Cross-validated predictions\n",
        "cv_preds = cross_val_predict(best_model, X, y, cv=cv, n_jobs=-1)\n",
        "print(\"Classification Report:\\n\", classification_report(y, cv_preds))\n",
        "cm = confusion_matrix(y, cv_preds)\n",
        "plt.figure(figsize=(6, 4))\n",
        "ax = sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "ax.set_title(f\"Confusion Matrix: {best_model_name}\")\n",
        "ax.set_xlabel(\"Predicted\")\n",
        "ax.set_ylabel(\"Actual\")\n",
        "plt.show()\n"
      "metadata": {},
        "## Discussion & Conclusion\n",
        "\n",
        "**Summary of approaches:**\n",
        "- Used **imputation + scaling + one-hot encoding** as a consistent preprocessing strategy.\n",
        "- Tested four distinct classifiers with different preprocessing choices and optional feature selection.\n",
        "- Evaluated performance with **Stratified 5-fold cross-validation** using balanced metrics for class imbalance.\n",
        "- Tuned hyperparameters for Logistic Regression and Random Forest using **Grid Search**.\n",
        "\n",
        "**Which worked best and why:**\n",
        "- The best model depends on the **F1 Macro** ranking in the results table.\n",
        "- **Tree-based models** (Random Forest, Gradient Boosting) often perform strongly when interactions and nonlinear patterns matter.\n",
        "- **Linear models** (Logistic Regression) benefit from feature selection and can be competitive with proper scaling.\n",
        "- **SVM (RBF)** can perform well but may be sensitive to scaling and can be slower on large datasets.\n",
        "\n",
        "This workflow demonstrates a broad evaluation of preprocessing choices, model families, and tuning strategies as requested.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    "language_info": {
      "name": "python",
      "version": "3.x"
  },
  "nbformat": 4,
  "nbformat_minor": 5